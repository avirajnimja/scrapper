# Unified Scraper API

A modular, scalable web scraping API built with FastAPI and Selenium. This project is designed to handle multiple scraping tasks across different websites through a single entry point.

## ğŸš€ Features
- **Modular Architecture**: Separate packages for different websites (SmartScout, etc.).
- **Unified API**: Single FastAPI entry point for all scraping tasks.
- **Dynamic Authentication**: Provide credentials per request for flexibility.
- **Base Scraper**: Shared utility class for browser management and automatic download handling.

## ğŸ“‚ Structure
```text
/scrapper
â”œâ”€â”€ main.py                 # FastAPI Application (Entry Point)
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ scrapers/               # Core Scraper Package
â”‚   â”œâ”€â”€ base_scraper.py     # Shared logic & driver setup
â”‚   â”œâ”€â”€ smartscout/         # SmartScout Package
â”‚   â”‚   â”œâ”€â”€ auth.py         # Website-specific login logic
â”‚   â”‚   â””â”€â”€ scrapers/       # Individual tasks
â”‚   â”‚       â”œâ”€â”€ niche_finder.py
â”‚   â”‚       â”œâ”€â”€ rank_maker.py
â”‚   â”‚       â””â”€â”€ ...         # Add more here
â”‚   â”œâ”€â”€ website2/           # Placeholder for next site
â”‚   â””â”€â”€ website3/           # Placeholder for next site
â””â”€â”€ downloads/              # Output folder for scraped files
```

## ğŸ› ï¸ Setup & Installation

1. **Create Virtual Environment**:
   ```bash
   python3 -m venv venv
   ```

2. **Activate Environment**:
   ```bash
   source venv/bin/activate  # Linux/Mac
   # OR
   .\venv\Scripts\activate   # Windows
   ```

3. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸƒ Running the API

Start the server using Uvicorn:
```bash
python main.py
```
By default, the API will be available at `http://localhost:8000`.

## ğŸ“¡ Usage (API Examples)

### Niche Finder Scrape
```bash
curl -X POST "http://localhost:8000/smartscout/niche-finder" \
     -H "Content-Type: application/json" \
     -d '{
           "search_text": "kitchen faucet",
           "username": "your_email@example.com",
           "password": "your_password"
         }'
```

The API will:
1. Initialize a browser session.
2. Log in using the provided credentials.
3. Perform the scrape.
4. Return the resulting CSV file directly in the response.

## ğŸ”§ Extending the Project
To add a new scraper for an existing website:
1. Create a new `.py` file in `scrapers/[website]/scrapers/`.
2. Implement your scraping logic.
3. Register the new endpoint in `main.py`.

To add a new website:
1. Create a new directory in `scrapers/`.
2. Add an `auth.py` for login.
3. Follow the same pattern as `smartscout/`.
